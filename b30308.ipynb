{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b52fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and setup.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix, classification_report)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Reproducibility.\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Path to dataset\n",
    "DATA_PATH = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c4db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training class distribution: Counter({0: 3622, 1: 1308})\n",
      "Original test class distribution: Counter({0: 1552, 1: 561})\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING\n",
    "\n",
    "# Loading dataset.\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "\n",
    "# Basic cleanup steps & target encoding.\n",
    "# Mapping target 'Churn' to binary 1/0\n",
    "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "\n",
    "# Handling TotalCharges blanks and converting to numeric.\n",
    "# In this dataset TotalCharges may be empty strings for customers with tenure=0\n",
    "# Converting to numeric coercing errors to NaN, then fill with median.\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'].replace(' ', np.nan), errors='coerce')\n",
    "total_median = df['TotalCharges'].median()  # choosing median to be robust\n",
    "df['TotalCharges'].fillna(total_median, inplace=True)\n",
    "\n",
    "\n",
    "# Identifying features.\n",
    "target_col = 'Churn'\n",
    "# Treat object dtype (excluding customerID) as categorical / nominal.\n",
    "drop_cols = ['customerID'] if 'customerID' in df.columns else []\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols + [target_col]]\n",
    "\n",
    "# Separate numeric vs categorical\n",
    "numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "# Confirm numeric cols exist; if not, detect numerics automatically\n",
    "numeric_cols = [c for c in numeric_cols if c in df.columns]\n",
    "# For categorical, use object dtype columns or explicitly exclude numeric\n",
    "categorical_cols = [c for c in feature_cols if c not in numeric_cols]\n",
    "\n",
    "\n",
    "# One-Hot Encoding for nominal categorical features.\n",
    "# Use drop_first=True to reduce collinearity.\n",
    "df_encoded = pd.get_dummies(df.drop(columns=drop_cols), columns=categorical_cols, drop_first=True)\n",
    "\n",
    "\n",
    "# Feature matrix and target vector\n",
    "X = df_encoded.drop(columns=[target_col])\n",
    "y = df_encoded[target_col]\n",
    "\n",
    "# Min-Max Scaling for numeric features (its very important to do this on the full data BEFORE the split so transforms are consistent.)\n",
    "# we scaled across the whole dataset for simplicity.\n",
    "scaler = MinMaxScaler()\n",
    "# finding the scaled numeric columns names in X (they exist unchanged since get_dummies didn't touch them)\n",
    "scale_cols = [c for c in numeric_cols if c in X.columns]\n",
    "X[scale_cols] = scaler.fit_transform(X[scale_cols])\n",
    "\n",
    "\n",
    "# Train/test split (70/30) with stratification on churn.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Original training class distribution:\", Counter(y_train))\n",
    "print(\"Original test class distribution:\", Counter(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1750e571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After SMOTE training class distribution: Counter({0: 3622, 1: 3622})\n"
     ]
    }
   ],
   "source": [
    "#HANDLING CLASS IMBALANCE WITH SMOTE\n",
    "\n",
    "# Applying SMOTE on training data only. Keep the test set untouched.\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"After SMOTE training class distribution:\", Counter(y_train_smote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60fe2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features by Mutual Information (pre-SMOTE training set):\n",
      "tenure                                  0.076137\n",
      "Contract_Two year                       0.060981\n",
      "InternetService_Fiber optic             0.054962\n",
      "PaymentMethod_Electronic check          0.051235\n",
      "MonthlyCharges                          0.044990\n",
      "TotalCharges                            0.038035\n",
      "DeviceProtection_No internet service    0.034879\n",
      "TechSupport_No internet service         0.034649\n",
      "StreamingMovies_No internet service     0.034344\n",
      "OnlineBackup_No internet service        0.030516\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#FEATURE SELECTION USING MUTUAL INFORMATION FOR LOGISTIC REGRESSION MODELING\n",
    "\n",
    "# Computing mutual information scores on the pre-SMOTE training set.\n",
    "mi_scores = mutual_info_classif(X_train, y_train, random_state=RANDOM_STATE)\n",
    "mi_series = pd.Series(mi_scores, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "# Top 10 most informative features.\n",
    "top_10_features = mi_series.head(10)\n",
    "print(\"Top 10 features by Mutual Information (pre-SMOTE training set):\")\n",
    "print(top_10_features)\n",
    "\n",
    "# For later steps we need the list (as Python list).\n",
    "top10_feature_list = top_10_features.index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c456d238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LR hyperparameters: {'C': 100, 'penalty': 'l1'}\n",
      "Best LR hyperparameters: {'C': 100, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "#LOGISTIC REGRESSION IMPLEMENTATION AND PARAMETER HYPERPARAMETER TUNING\n",
    "\n",
    "# Prepare training data for Logistic Regression using SMOTE-balanced training set\n",
    "# Using only the top 10 features found with MI\n",
    "X_train_lr = X_train_smote[top10_feature_list]\n",
    "X_test_lr  = X_test[top10_feature_list]  # test set still from holdout (not SMOTE)\n",
    "\n",
    "# Defining model and hyperparameter grid.\n",
    "lr = LogisticRegression(solver='liblinear', random_state=RANDOM_STATE, max_iter=1000)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(lr, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train_lr, y_train_smote)\n",
    "\n",
    "print(\"Best LR hyperparameters:\", grid.best_params_)\n",
    "best_lr = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3721564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "102/102 - 3s - 30ms/step - AUC: 0.8014 - loss: 0.5491 - val_AUC: 0.0000e+00 - val_loss: 0.6271\n",
      "Epoch 2/100\n",
      "102/102 - 3s - 30ms/step - AUC: 0.8014 - loss: 0.5491 - val_AUC: 0.0000e+00 - val_loss: 0.6271\n",
      "Epoch 2/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8508 - loss: 0.4767 - val_AUC: 0.0000e+00 - val_loss: 0.6111\n",
      "Epoch 3/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8508 - loss: 0.4767 - val_AUC: 0.0000e+00 - val_loss: 0.6111\n",
      "Epoch 3/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8576 - loss: 0.4650 - val_AUC: 0.0000e+00 - val_loss: 0.5392\n",
      "Epoch 4/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8576 - loss: 0.4650 - val_AUC: 0.0000e+00 - val_loss: 0.5392\n",
      "Epoch 4/100\n",
      "102/102 - 0s - 3ms/step - AUC: 0.8622 - loss: 0.4582 - val_AUC: 0.0000e+00 - val_loss: 0.4837\n",
      "Epoch 5/100\n",
      "102/102 - 0s - 3ms/step - AUC: 0.8622 - loss: 0.4582 - val_AUC: 0.0000e+00 - val_loss: 0.4837\n",
      "Epoch 5/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8649 - loss: 0.4539 - val_AUC: 0.0000e+00 - val_loss: 0.5440\n",
      "Epoch 6/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8649 - loss: 0.4539 - val_AUC: 0.0000e+00 - val_loss: 0.5440\n",
      "Epoch 6/100\n",
      "102/102 - 1s - 6ms/step - AUC: 0.8689 - loss: 0.4477 - val_AUC: 0.0000e+00 - val_loss: 0.5144\n",
      "Epoch 7/100\n",
      "102/102 - 1s - 6ms/step - AUC: 0.8689 - loss: 0.4477 - val_AUC: 0.0000e+00 - val_loss: 0.5144\n",
      "Epoch 7/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8701 - loss: 0.4455 - val_AUC: 0.0000e+00 - val_loss: 0.5718\n",
      "Epoch 8/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8701 - loss: 0.4455 - val_AUC: 0.0000e+00 - val_loss: 0.5718\n",
      "Epoch 8/100\n",
      "102/102 - 1s - 8ms/step - AUC: 0.8731 - loss: 0.4408 - val_AUC: 0.0000e+00 - val_loss: 0.5498\n",
      "Epoch 9/100\n",
      "102/102 - 1s - 8ms/step - AUC: 0.8731 - loss: 0.4408 - val_AUC: 0.0000e+00 - val_loss: 0.5498\n",
      "Epoch 9/100\n",
      "102/102 - 1s - 7ms/step - AUC: 0.8756 - loss: 0.4364 - val_AUC: 0.0000e+00 - val_loss: 0.3965\n",
      "Epoch 10/100\n",
      "102/102 - 1s - 7ms/step - AUC: 0.8756 - loss: 0.4364 - val_AUC: 0.0000e+00 - val_loss: 0.3965\n",
      "Epoch 10/100\n",
      "102/102 - 1s - 7ms/step - AUC: 0.8783 - loss: 0.4325 - val_AUC: 0.0000e+00 - val_loss: 0.4306\n",
      "Epoch 11/100\n",
      "102/102 - 1s - 7ms/step - AUC: 0.8783 - loss: 0.4325 - val_AUC: 0.0000e+00 - val_loss: 0.4306\n",
      "Epoch 11/100\n",
      "102/102 - 0s - 5ms/step - AUC: 0.8789 - loss: 0.4306 - val_AUC: 0.0000e+00 - val_loss: 0.5087\n",
      "Epoch 12/100\n",
      "102/102 - 0s - 5ms/step - AUC: 0.8789 - loss: 0.4306 - val_AUC: 0.0000e+00 - val_loss: 0.5087\n",
      "Epoch 12/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8838 - loss: 0.4233 - val_AUC: 0.0000e+00 - val_loss: 0.4384\n",
      "Epoch 13/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8838 - loss: 0.4233 - val_AUC: 0.0000e+00 - val_loss: 0.4384\n",
      "Epoch 13/100\n",
      "102/102 - 0s - 3ms/step - AUC: 0.8864 - loss: 0.4193 - val_AUC: 0.0000e+00 - val_loss: 0.4402\n",
      "Epoch 14/100\n",
      "102/102 - 0s - 3ms/step - AUC: 0.8864 - loss: 0.4193 - val_AUC: 0.0000e+00 - val_loss: 0.4402\n",
      "Epoch 14/100\n",
      "102/102 - 0s - 3ms/step - AUC: 0.8877 - loss: 0.4168 - val_AUC: 0.0000e+00 - val_loss: 0.5100\n",
      "102/102 - 0s - 3ms/step - AUC: 0.8877 - loss: 0.4168 - val_AUC: 0.0000e+00 - val_loss: 0.5100\n"
     ]
    }
   ],
   "source": [
    "# DEEP NEURAL NETWORK IMPLEMENTATION\n",
    "\n",
    "# Preparing full training set (SMOTE-balanced) and full test set (holdout) for DNN\n",
    "X_train_dnn = X_train_smote.astype('float32').values  # convert to float32 and numpy array\n",
    "y_train_dnn = y_train_smote.astype('float32').values\n",
    "X_test_dnn  = X_test.astype('float32').values\n",
    "y_test_dnn  = y_test.astype('float32').values\n",
    "\n",
    "input_dim = X_train_dnn.shape[1]\n",
    "\n",
    "# Building the Keras model as requested:\n",
    "def build_dnn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(input_dim,)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # binary output\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "dnn_model = build_dnn_model(input_dim)\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the DNN\n",
    "history = dnn_model.fit(\n",
    "    X_train_dnn, y_train_dnn,\n",
    "    validation_split=0.10,  # 10% of training used for validation\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "070d90a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "DNN Confusion Matrix (rows: true class 0/1, cols: predicted 0/1):\n",
      "[[1143  409]\n",
      " [ 125  436]]\n",
      "DNN Confusion Matrix (rows: true class 0/1, cols: predicted 0/1):\n",
      "[[1143  409]\n",
      " [ 125  436]]\n"
     ]
    }
   ],
   "source": [
    "#COMPARATIVE EVALUATION OF MODELS ON TEST SET\n",
    "\n",
    "# Logistic Regression predictions\n",
    "# For LR we used only top10 features\n",
    "y_pred_lr = best_lr.predict(X_test_lr)\n",
    "y_proba_lr = best_lr.predict_proba(X_test_lr)[:, 1]\n",
    "\n",
    "# LR metrics\n",
    "lr_acc = accuracy_score(y_test, y_pred_lr)\n",
    "lr_prec = precision_score(y_test, y_pred_lr)\n",
    "lr_rec = recall_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "lr_auc = roc_auc_score(y_test, y_proba_lr)\n",
    "\n",
    "# DNN predictions\n",
    "y_proba_dnn = dnn_model.predict(X_test_dnn).ravel()\n",
    "y_pred_dnn = (y_proba_dnn >= 0.5).astype(int)\n",
    "\n",
    "# DNN metrics\n",
    "dnn_acc = accuracy_score(y_test_dnn, y_pred_dnn)\n",
    "dnn_prec = precision_score(y_test_dnn, y_pred_dnn)\n",
    "dnn_rec = recall_score(y_test_dnn, y_pred_dnn)\n",
    "dnn_f1 = f1_score(y_test_dnn, y_pred_dnn)\n",
    "dnn_auc = roc_auc_score(y_test_dnn, y_proba_dnn)\n",
    "\n",
    "# Confusion matrix for DNN\n",
    "dnn_cm = confusion_matrix(y_test_dnn, y_pred_dnn)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"DNN Confusion Matrix (rows: true class 0/1, cols: predicted 0/1):\")\n",
    "print(dnn_cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93be7eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Comparative Evaluation Results (Test Set / Holdout 30%)\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1140\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tabulate'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Show table.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m### Comparative Evaluation Results (Test Set / Holdout 30\u001b[39m\u001b[33m%\u001b[39m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Also print best LR hyperparameters and top-10 features found earlier\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBest Logistic Regression hyperparameters found by GridSearchCV:\u001b[39m\u001b[33m\"\u001b[39m, grid.best_params_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:2994\u001b[39m, in \u001b[36mDataFrame.to_markdown\u001b[39m\u001b[34m(self, buf, mode, index, storage_options, **kwargs)\u001b[39m\n\u001b[32m   2992\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mtablefmt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2993\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mshowindex\u001b[39m\u001b[33m\"\u001b[39m, index)\n\u001b[32m-> \u001b[39m\u001b[32m2994\u001b[39m tabulate = \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtabulate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2995\u001b[39m result = tabulate.tabulate(\u001b[38;5;28mself\u001b[39m, **kwargs)\n\u001b[32m   2996\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate."
     ]
    }
   ],
   "source": [
    "# Building a DataFrame for a clean table\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression (top 10 features)', 'Deep Neural Network (full features)'],\n",
    "    'Accuracy': [lr_acc, dnn_acc],\n",
    "    'Precision': [lr_prec, dnn_prec],\n",
    "    'Recall': [lr_rec, dnn_rec],\n",
    "    'F1-score': [lr_f1, dnn_f1],\n",
    "    'ROC-AUC': [lr_auc, dnn_auc]\n",
    "})\n",
    "\n",
    "# Round metrics for nicer display.\n",
    "results[['Accuracy','Precision','Recall','F1-score','ROC-AUC']] = results[['Accuracy','Precision','Recall','F1-score','ROC-AUC']].round(4)\n",
    "\n",
    "# Show table.\n",
    "print(\"\\n### Comparative Evaluation Results (Test Set / Holdout 30%)\\n\")\n",
    "print(results.to_markdown(index=False))\n",
    "\n",
    "# Also print best LR hyperparameters and top-10 features found earlier\n",
    "print(\"\\nBest Logistic Regression hyperparameters found by GridSearchCV:\", grid.best_params_)\n",
    "print(\"\\nTop 10 features selected by Mutual Information (pre-SMOTE training set):\")\n",
    "for i, feat in enumerate(top10_feature_list, 1):\n",
    "    print(f\"{i}. {feat}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
