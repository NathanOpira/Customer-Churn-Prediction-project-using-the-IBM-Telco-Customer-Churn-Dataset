{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b52fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and setup.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix, classification_report)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Reproducibility.\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Path to dataset\n",
    "DATA_PATH = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2c4db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training class distribution: Counter({0: 3622, 1: 1308})\n",
      "Original test class distribution: Counter({0: 1552, 1: 561})\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING\n",
    "\n",
    "# Loading dataset.\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "\n",
    "# Basic cleanup steps & target encoding.\n",
    "# Mapping target 'Churn' to binary 1/0\n",
    "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "\n",
    "# Handling TotalCharges blanks and converting to numeric.\n",
    "# In this dataset TotalCharges may be empty strings for customers with tenure=0\n",
    "# Converting to numeric coercing errors to NaN, then fill with median.\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'].replace(' ', np.nan), errors='coerce')\n",
    "total_median = df['TotalCharges'].median()  # choosing median to be robust\n",
    "df['TotalCharges'].fillna(total_median, inplace=True)\n",
    "\n",
    "\n",
    "# Identifying features.\n",
    "target_col = 'Churn'\n",
    "# Treat object dtype (excluding customerID) as categorical / nominal.\n",
    "drop_cols = ['customerID'] if 'customerID' in df.columns else []\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols + [target_col]]\n",
    "\n",
    "# Separate numeric vs categorical\n",
    "numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "# Confirm numeric cols exist; if not, detect numerics automatically\n",
    "numeric_cols = [c for c in numeric_cols if c in df.columns]\n",
    "# For categorical, use object dtype columns or explicitly exclude numeric\n",
    "categorical_cols = [c for c in feature_cols if c not in numeric_cols]\n",
    "\n",
    "\n",
    "# One-Hot Encoding for nominal categorical features.\n",
    "# Use drop_first=True to reduce collinearity.\n",
    "df_encoded = pd.get_dummies(df.drop(columns=drop_cols), columns=categorical_cols, drop_first=True)\n",
    "\n",
    "\n",
    "# Feature matrix and target vector\n",
    "X = df_encoded.drop(columns=[target_col])\n",
    "y = df_encoded[target_col]\n",
    "\n",
    "# Min-Max Scaling for numeric features (its very important to do this on the full data BEFORE the split so transforms are consistent.)\n",
    "# we scaled across the whole dataset for simplicity.\n",
    "scaler = MinMaxScaler()\n",
    "# finding the scaled numeric columns names in X (they exist unchanged since get_dummies didn't touch them)\n",
    "scale_cols = [c for c in numeric_cols if c in X.columns]\n",
    "X[scale_cols] = scaler.fit_transform(X[scale_cols])\n",
    "\n",
    "\n",
    "# Train/test split (70/30) with stratification on churn.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Original training class distribution:\", Counter(y_train))\n",
    "print(\"Original test class distribution:\", Counter(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1750e571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After SMOTE training class distribution: Counter({0: 3622, 1: 3622})\n"
     ]
    }
   ],
   "source": [
    "#HANDLING CLASS IMBALANCE WITH SMOTE\n",
    "\n",
    "# Applying SMOTE on training data only. Keep the test set untouched.\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"After SMOTE training class distribution:\", Counter(y_train_smote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f60fe2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features by Mutual Information (pre-SMOTE training set):\n",
      "tenure                                  0.076137\n",
      "Contract_Two year                       0.060981\n",
      "InternetService_Fiber optic             0.054962\n",
      "PaymentMethod_Electronic check          0.051235\n",
      "MonthlyCharges                          0.044990\n",
      "TotalCharges                            0.038035\n",
      "DeviceProtection_No internet service    0.034879\n",
      "TechSupport_No internet service         0.034649\n",
      "StreamingMovies_No internet service     0.034344\n",
      "OnlineBackup_No internet service        0.030516\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#FEATURE SELECTION USING MUTUAL INFORMATION FOR LOGISTIC REGRESSION MODELING\n",
    "\n",
    "# Computing mutual information scores on the pre-SMOTE training set.\n",
    "mi_scores = mutual_info_classif(X_train, y_train, random_state=RANDOM_STATE)\n",
    "mi_series = pd.Series(mi_scores, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "# Top 10 most informative features.\n",
    "top_10_features = mi_series.head(10)\n",
    "print(\"Top 10 features by Mutual Information (pre-SMOTE training set):\")\n",
    "print(top_10_features)\n",
    "\n",
    "# For later steps we need the list (as Python list).\n",
    "top10_feature_list = top_10_features.index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c456d238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LR hyperparameters: {'C': 100, 'penalty': 'l1'}\n",
      "Best LR hyperparameters: {'C': 100, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "#LOGISTIC REGRESSION IMPLEMENTATION AND PARAMETER HYPERPARAMETER TUNING\n",
    "\n",
    "# Prepare training data for Logistic Regression using SMOTE-balanced training set\n",
    "# Using only the top 10 features found with MI\n",
    "X_train_lr = X_train_smote[top10_feature_list]\n",
    "X_test_lr  = X_test[top10_feature_list]  # test set still from holdout (not SMOTE)\n",
    "\n",
    "# Defining model and hyperparameter grid.\n",
    "lr = LogisticRegression(solver='liblinear', random_state=RANDOM_STATE, max_iter=1000)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(lr, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train_lr, y_train_smote)\n",
    "\n",
    "print(\"Best LR hyperparameters:\", grid.best_params_)\n",
    "best_lr = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3721564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "102/102 - 3s - 30ms/step - AUC: 0.8107 - loss: 0.5351 - val_AUC: 0.0000e+00 - val_loss: 0.6251\n",
      "Epoch 2/100\n",
      "102/102 - 3s - 30ms/step - AUC: 0.8107 - loss: 0.5351 - val_AUC: 0.0000e+00 - val_loss: 0.6251\n",
      "Epoch 2/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8534 - loss: 0.4724 - val_AUC: 0.0000e+00 - val_loss: 0.5791\n",
      "Epoch 3/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8534 - loss: 0.4724 - val_AUC: 0.0000e+00 - val_loss: 0.5791\n",
      "Epoch 3/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8602 - loss: 0.4613 - val_AUC: 0.0000e+00 - val_loss: 0.5185\n",
      "Epoch 4/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8602 - loss: 0.4613 - val_AUC: 0.0000e+00 - val_loss: 0.5185\n",
      "Epoch 4/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8646 - loss: 0.4544 - val_AUC: 0.0000e+00 - val_loss: 0.5232\n",
      "Epoch 5/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8646 - loss: 0.4544 - val_AUC: 0.0000e+00 - val_loss: 0.5232\n",
      "Epoch 5/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8679 - loss: 0.4493 - val_AUC: 0.0000e+00 - val_loss: 0.5400\n",
      "Epoch 6/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8679 - loss: 0.4493 - val_AUC: 0.0000e+00 - val_loss: 0.5400\n",
      "Epoch 6/100\n",
      "102/102 - 1s - 5ms/step - AUC: 0.8712 - loss: 0.4442 - val_AUC: 0.0000e+00 - val_loss: 0.4971\n",
      "Epoch 7/100\n",
      "102/102 - 1s - 5ms/step - AUC: 0.8712 - loss: 0.4442 - val_AUC: 0.0000e+00 - val_loss: 0.4971\n",
      "Epoch 7/100\n",
      "102/102 - 0s - 5ms/step - AUC: 0.8728 - loss: 0.4417 - val_AUC: 0.0000e+00 - val_loss: 0.4918\n",
      "Epoch 8/100\n",
      "102/102 - 0s - 5ms/step - AUC: 0.8728 - loss: 0.4417 - val_AUC: 0.0000e+00 - val_loss: 0.4918\n",
      "Epoch 8/100\n",
      "102/102 - 1s - 6ms/step - AUC: 0.8759 - loss: 0.4369 - val_AUC: 0.0000e+00 - val_loss: 0.4871\n",
      "Epoch 9/100\n",
      "102/102 - 1s - 6ms/step - AUC: 0.8759 - loss: 0.4369 - val_AUC: 0.0000e+00 - val_loss: 0.4871\n",
      "Epoch 9/100\n",
      "102/102 - 1s - 7ms/step - AUC: 0.8786 - loss: 0.4328 - val_AUC: 0.0000e+00 - val_loss: 0.5108\n",
      "Epoch 10/100\n",
      "102/102 - 1s - 7ms/step - AUC: 0.8786 - loss: 0.4328 - val_AUC: 0.0000e+00 - val_loss: 0.5108\n",
      "Epoch 10/100\n",
      "102/102 - 1s - 6ms/step - AUC: 0.8802 - loss: 0.4298 - val_AUC: 0.0000e+00 - val_loss: 0.4645\n",
      "Epoch 11/100\n",
      "102/102 - 1s - 6ms/step - AUC: 0.8802 - loss: 0.4298 - val_AUC: 0.0000e+00 - val_loss: 0.4645\n",
      "Epoch 11/100\n",
      "102/102 - 1s - 6ms/step - AUC: 0.8819 - loss: 0.4268 - val_AUC: 0.0000e+00 - val_loss: 0.4566\n",
      "Epoch 12/100\n",
      "102/102 - 1s - 6ms/step - AUC: 0.8819 - loss: 0.4268 - val_AUC: 0.0000e+00 - val_loss: 0.4566\n",
      "Epoch 12/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8833 - loss: 0.4248 - val_AUC: 0.0000e+00 - val_loss: 0.4457\n",
      "Epoch 13/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8833 - loss: 0.4248 - val_AUC: 0.0000e+00 - val_loss: 0.4457\n",
      "Epoch 13/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8856 - loss: 0.4208 - val_AUC: 0.0000e+00 - val_loss: 0.4527\n",
      "Epoch 14/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8856 - loss: 0.4208 - val_AUC: 0.0000e+00 - val_loss: 0.4527\n",
      "Epoch 14/100\n",
      "102/102 - 1s - 6ms/step - AUC: 0.8874 - loss: 0.4177 - val_AUC: 0.0000e+00 - val_loss: 0.5208\n",
      "Epoch 15/100\n",
      "102/102 - 1s - 6ms/step - AUC: 0.8874 - loss: 0.4177 - val_AUC: 0.0000e+00 - val_loss: 0.5208\n",
      "Epoch 15/100\n",
      "102/102 - 1s - 6ms/step - AUC: 0.8889 - loss: 0.4152 - val_AUC: 0.0000e+00 - val_loss: 0.4476\n",
      "Epoch 16/100\n",
      "102/102 - 1s - 6ms/step - AUC: 0.8889 - loss: 0.4152 - val_AUC: 0.0000e+00 - val_loss: 0.4476\n",
      "Epoch 16/100\n",
      "102/102 - 1s - 6ms/step - AUC: 0.8903 - loss: 0.4129 - val_AUC: 0.0000e+00 - val_loss: 0.4859\n",
      "Epoch 17/100\n",
      "102/102 - 1s - 6ms/step - AUC: 0.8903 - loss: 0.4129 - val_AUC: 0.0000e+00 - val_loss: 0.4859\n",
      "Epoch 17/100\n",
      "102/102 - 1s - 5ms/step - AUC: 0.8921 - loss: 0.4097 - val_AUC: 0.0000e+00 - val_loss: 0.4520\n",
      "102/102 - 1s - 5ms/step - AUC: 0.8921 - loss: 0.4097 - val_AUC: 0.0000e+00 - val_loss: 0.4520\n"
     ]
    }
   ],
   "source": [
    "# DEEP NEURAL NETWORK IMPLEMENTATION\n",
    "\n",
    "# Preparing full training set (SMOTE-balanced) and full test set (holdout) for DNN\n",
    "X_train_dnn = X_train_smote.astype('float32').values  # convert to float32 and numpy array\n",
    "y_train_dnn = y_train_smote.astype('float32').values\n",
    "X_test_dnn  = X_test.astype('float32').values\n",
    "y_test_dnn  = y_test.astype('float32').values\n",
    "\n",
    "input_dim = X_train_dnn.shape[1]\n",
    "\n",
    "# Building the Keras model as requested:\n",
    "def build_dnn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(input_dim,)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # binary output\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "dnn_model = build_dnn_model(input_dim)\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the DNN\n",
    "history = dnn_model.fit(\n",
    "    X_train_dnn, y_train_dnn,\n",
    "    validation_split=0.10,  # 10% of training used for validation\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "070d90a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "DNN Confusion Matrix (rows: true class 0/1, cols: predicted 0/1):\n",
      "[[1202  350]\n",
      " [ 153  408]]\n",
      "DNN Confusion Matrix (rows: true class 0/1, cols: predicted 0/1):\n",
      "[[1202  350]\n",
      " [ 153  408]]\n"
     ]
    }
   ],
   "source": [
    "#COMPARATIVE EVALUATION OF MODELS ON TEST SET\n",
    "\n",
    "# Logistic Regression predictions\n",
    "# For LR we used only top10 features\n",
    "y_pred_lr = best_lr.predict(X_test_lr)\n",
    "y_proba_lr = best_lr.predict_proba(X_test_lr)[:, 1]\n",
    "\n",
    "# LR metrics\n",
    "lr_acc = accuracy_score(y_test, y_pred_lr)\n",
    "lr_prec = precision_score(y_test, y_pred_lr)\n",
    "lr_rec = recall_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "lr_auc = roc_auc_score(y_test, y_proba_lr)\n",
    "\n",
    "# DNN predictions\n",
    "y_proba_dnn = dnn_model.predict(X_test_dnn).ravel()\n",
    "y_pred_dnn = (y_proba_dnn >= 0.5).astype(int)\n",
    "\n",
    "# DNN metrics\n",
    "dnn_acc = accuracy_score(y_test_dnn, y_pred_dnn)\n",
    "dnn_prec = precision_score(y_test_dnn, y_pred_dnn)\n",
    "dnn_rec = recall_score(y_test_dnn, y_pred_dnn)\n",
    "dnn_f1 = f1_score(y_test_dnn, y_pred_dnn)\n",
    "dnn_auc = roc_auc_score(y_test_dnn, y_proba_dnn)\n",
    "\n",
    "# Confusion matrix for DNN\n",
    "dnn_cm = confusion_matrix(y_test_dnn, y_pred_dnn)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"DNN Confusion Matrix (rows: true class 0/1, cols: predicted 0/1):\")\n",
    "print(dnn_cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93be7eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Comparative Evaluation Results (Test Set / Holdout 30%)\n",
      "\n",
      "                                Model  Accuracy  Precision  Recall  F1-score  ROC-AUC\n",
      "Logistic Regression (top 10 features)    0.7378     0.5039  0.8093    0.6211   0.8354\n",
      "  Deep Neural Network (full features)    0.7619     0.5383  0.7273    0.6187   0.8322\n",
      "\n",
      "Best Logistic Regression hyperparameters found by GridSearchCV: {'C': 100, 'penalty': 'l1'}\n",
      "\n",
      "Top 10 features selected by Mutual Information (pre-SMOTE training set):\n",
      "1. tenure\n",
      "2. Contract_Two year\n",
      "3. InternetService_Fiber optic\n",
      "4. PaymentMethod_Electronic check\n",
      "5. MonthlyCharges\n",
      "6. TotalCharges\n",
      "7. DeviceProtection_No internet service\n",
      "8. TechSupport_No internet service\n",
      "9. StreamingMovies_No internet service\n",
      "10. OnlineBackup_No internet service\n"
     ]
    }
   ],
   "source": [
    "# Building a DataFrame for a clean table\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression (top 10 features)', 'Deep Neural Network (full features)'],\n",
    "    'Accuracy': [lr_acc, dnn_acc],\n",
    "    'Precision': [lr_prec, dnn_prec],\n",
    "    'Recall': [lr_rec, dnn_rec],\n",
    "    'F1-score': [lr_f1, dnn_f1],\n",
    "    'ROC-AUC': [lr_auc, dnn_auc]\n",
    "})\n",
    "\n",
    "# Round metrics for nicer display.\n",
    "results[['Accuracy','Precision','Recall','F1-score','ROC-AUC']] = results[['Accuracy','Precision','Recall','F1-score','ROC-AUC']].round(4)\n",
    "\n",
    "# Show table.\n",
    "print(\"\\n### Comparative Evaluation Results (Test Set / Holdout 30%)\\n\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Also print best LR hyperparameters and top-10 features found earlier\n",
    "print(\"\\nBest Logistic Regression hyperparameters found by GridSearchCV:\", grid.best_params_)\n",
    "print(\"\\nTop 10 features selected by Mutual Information (pre-SMOTE training set):\")\n",
    "for i, feat in enumerate(top10_feature_list, 1):\n",
    "    print(f\"{i}. {feat}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
