{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503ebc5f",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction Analysis\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a comprehensive machine learning analysis to predict customer churn using the IBM Telco Customer Churn dataset. The analysis compares two approaches:\n",
    "- **Logistic Regression** with feature selection and hyperparameter tuning\n",
    "- **Deep Neural Network** with early stopping regularization\n",
    "\n",
    "## Key Workflow\n",
    "1. **Data Preprocessing** - Loading, cleaning, encoding, and scaling\n",
    "2. **Class Imbalance Handling** - SMOTE for balanced training data\n",
    "3. **Feature Selection** - Mutual Information for identifying top predictors\n",
    "4. **Model Training** - LR with GridSearchCV and DNN with Keras\n",
    "5. **Evaluation** - Comprehensive metrics comparison\n",
    "6. **Model Persistence** - Saving models and preprocessing objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b52fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and setup.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix, classification_report)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Reproducibility.\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Path to dataset\n",
    "DATA_PATH = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bedaf5f",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "Import all necessary libraries and set configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf2c4db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training class distribution: Counter({0: 3622, 1: 1308})\n",
      "Original test class distribution: Counter({0: 1552, 1: 561})\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING\n",
    "\n",
    "# Loading dataset.\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "\n",
    "# Basic cleanup steps & target encoding.\n",
    "# Mapping target 'Churn' to binary 1/0\n",
    "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "\n",
    "# Handling TotalCharges blanks and converting to numeric.\n",
    "# In this dataset TotalCharges may be empty strings for customers with tenure=0\n",
    "# Converting to numeric coercing errors to NaN, then fill with median.\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'].replace(' ', np.nan), errors='coerce')\n",
    "total_median = df['TotalCharges'].median()  # choosing median to be robust\n",
    "df['TotalCharges'].fillna(total_median, inplace=True)\n",
    "\n",
    "\n",
    "# Identifying features.\n",
    "target_col = 'Churn'\n",
    "# Treat object dtype (excluding customerID) as categorical / nominal.\n",
    "drop_cols = ['customerID'] if 'customerID' in df.columns else []\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols + [target_col]]\n",
    "\n",
    "# Separate numeric vs categorical\n",
    "numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "# Confirm numeric cols exist; if not, detect numerics automatically\n",
    "numeric_cols = [c for c in numeric_cols if c in df.columns]\n",
    "# For categorical, use object dtype columns or explicitly exclude numeric\n",
    "categorical_cols = [c for c in feature_cols if c not in numeric_cols]\n",
    "\n",
    "\n",
    "# One-Hot Encoding for nominal categorical features.\n",
    "# Use drop_first=True to reduce collinearity.\n",
    "df_encoded = pd.get_dummies(df.drop(columns=drop_cols), columns=categorical_cols, drop_first=True)\n",
    "\n",
    "\n",
    "# Feature matrix and target vector\n",
    "X = df_encoded.drop(columns=[target_col])\n",
    "y = df_encoded[target_col]\n",
    "\n",
    "# Min-Max Scaling for numeric features (its very important to do this on the full data BEFORE the split so transforms are consistent.)\n",
    "# we scaled across the whole dataset for simplicity.\n",
    "scaler = MinMaxScaler()\n",
    "# finding the scaled numeric columns names in X (they exist unchanged since get_dummies didn't touch them)\n",
    "scale_cols = [c for c in numeric_cols if c in X.columns]\n",
    "X[scale_cols] = scaler.fit_transform(X[scale_cols])\n",
    "\n",
    "\n",
    "# Train/test split (70/30) with stratification on churn.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Original training class distribution:\", Counter(y_train))\n",
    "print(\"Original test class distribution:\", Counter(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee09e36",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "Clean the dataset, handle missing values, encode categorical features, and scale numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1750e571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After SMOTE training class distribution: Counter({0: 3622, 1: 3622})\n"
     ]
    }
   ],
   "source": [
    "#HANDLING CLASS IMBALANCE WITH SMOTE\n",
    "\n",
    "# Applying SMOTE on training data only. Keep the test set untouched.\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"After SMOTE training class distribution:\", Counter(y_train_smote))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4ee47",
   "metadata": {},
   "source": [
    "## 3. Class Imbalance Handling\n",
    "Apply SMOTE (Synthetic Minority Oversampling Technique) to balance the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f60fe2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features by Mutual Information (pre-SMOTE training set):\n",
      "tenure                                  0.076137\n",
      "Contract_Two year                       0.060981\n",
      "InternetService_Fiber optic             0.054962\n",
      "PaymentMethod_Electronic check          0.051235\n",
      "MonthlyCharges                          0.044990\n",
      "TotalCharges                            0.038035\n",
      "DeviceProtection_No internet service    0.034879\n",
      "TechSupport_No internet service         0.034649\n",
      "StreamingMovies_No internet service     0.034344\n",
      "OnlineBackup_No internet service        0.030516\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#FEATURE SELECTION USING MUTUAL INFORMATION FOR LOGISTIC REGRESSION MODELING\n",
    "\n",
    "# Computing mutual information scores on the pre-SMOTE training set.\n",
    "mi_scores = mutual_info_classif(X_train, y_train, random_state=RANDOM_STATE)\n",
    "mi_series = pd.Series(mi_scores, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "# Top 10 most informative features.\n",
    "top_10_features = mi_series.head(10)\n",
    "print(\"Top 10 features by Mutual Information (pre-SMOTE training set):\")\n",
    "print(top_10_features)\n",
    "\n",
    "# For later steps we need the list (as Python list).\n",
    "top10_feature_list = top_10_features.index.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c917aa",
   "metadata": {},
   "source": [
    "## 4. Feature Selection\n",
    "Identify the most informative features using Mutual Information scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c456d238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LR hyperparameters: {'C': 100, 'penalty': 'l1'}\n",
      "Best LR hyperparameters: {'C': 100, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "#LOGISTIC REGRESSION IMPLEMENTATION AND PARAMETER HYPERPARAMETER TUNING\n",
    "\n",
    "# Prepare training data for Logistic Regression using SMOTE-balanced training set\n",
    "# Using only the top 10 features found with MI\n",
    "X_train_lr = X_train_smote[top10_feature_list]\n",
    "X_test_lr  = X_test[top10_feature_list]  # test set still from holdout (not SMOTE)\n",
    "\n",
    "# Defining model and hyperparameter grid.\n",
    "lr = LogisticRegression(solver='liblinear', random_state=RANDOM_STATE, max_iter=1000)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(lr, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train_lr, y_train_smote)\n",
    "\n",
    "print(\"Best LR hyperparameters:\", grid.best_params_)\n",
    "best_lr = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bfc7b9",
   "metadata": {},
   "source": [
    "## 5. Logistic Regression Model\n",
    "Train Logistic Regression with hyperparameter tuning using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3721564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "102/102 - 4s - 36ms/step - AUC: 0.8165 - loss: 0.5260 - val_AUC: 0.0000e+00 - val_loss: 0.4710\n",
      "Epoch 2/100\n",
      "102/102 - 4s - 36ms/step - AUC: 0.8165 - loss: 0.5260 - val_AUC: 0.0000e+00 - val_loss: 0.4710\n",
      "Epoch 2/100\n",
      "102/102 - 1s - 5ms/step - AUC: 0.8527 - loss: 0.4734 - val_AUC: 0.0000e+00 - val_loss: 0.6475\n",
      "Epoch 3/100\n",
      "102/102 - 1s - 5ms/step - AUC: 0.8527 - loss: 0.4734 - val_AUC: 0.0000e+00 - val_loss: 0.6475\n",
      "Epoch 3/100\n",
      "102/102 - 1s - 5ms/step - AUC: 0.8597 - loss: 0.4626 - val_AUC: 0.0000e+00 - val_loss: 0.5703\n",
      "Epoch 4/100\n",
      "102/102 - 1s - 5ms/step - AUC: 0.8597 - loss: 0.4626 - val_AUC: 0.0000e+00 - val_loss: 0.5703\n",
      "Epoch 4/100\n",
      "102/102 - 1s - 5ms/step - AUC: 0.8647 - loss: 0.4551 - val_AUC: 0.0000e+00 - val_loss: 0.5857\n",
      "Epoch 5/100\n",
      "102/102 - 1s - 5ms/step - AUC: 0.8647 - loss: 0.4551 - val_AUC: 0.0000e+00 - val_loss: 0.5857\n",
      "Epoch 5/100\n",
      "102/102 - 0s - 3ms/step - AUC: 0.8679 - loss: 0.4497 - val_AUC: 0.0000e+00 - val_loss: 0.4346\n",
      "Epoch 6/100\n",
      "102/102 - 0s - 3ms/step - AUC: 0.8679 - loss: 0.4497 - val_AUC: 0.0000e+00 - val_loss: 0.4346\n",
      "Epoch 6/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8705 - loss: 0.4454 - val_AUC: 0.0000e+00 - val_loss: 0.5439\n",
      "Epoch 7/100\n",
      "102/102 - 0s - 4ms/step - AUC: 0.8705 - loss: 0.4454 - val_AUC: 0.0000e+00 - val_loss: 0.5439\n",
      "Epoch 7/100\n",
      "102/102 - 0s - 5ms/step - AUC: 0.8746 - loss: 0.4395 - val_AUC: 0.0000e+00 - val_loss: 0.4911\n",
      "Epoch 8/100\n",
      "102/102 - 0s - 5ms/step - AUC: 0.8746 - loss: 0.4395 - val_AUC: 0.0000e+00 - val_loss: 0.4911\n",
      "Epoch 8/100\n",
      "102/102 - 0s - 5ms/step - AUC: 0.8771 - loss: 0.4351 - val_AUC: 0.0000e+00 - val_loss: 0.4737\n",
      "Epoch 9/100\n",
      "102/102 - 0s - 5ms/step - AUC: 0.8771 - loss: 0.4351 - val_AUC: 0.0000e+00 - val_loss: 0.4737\n",
      "Epoch 9/100\n",
      "102/102 - 0s - 5ms/step - AUC: 0.8796 - loss: 0.4312 - val_AUC: 0.0000e+00 - val_loss: 0.4613\n",
      "Epoch 10/100\n",
      "102/102 - 0s - 5ms/step - AUC: 0.8796 - loss: 0.4312 - val_AUC: 0.0000e+00 - val_loss: 0.4613\n",
      "Epoch 10/100\n",
      "102/102 - 0s - 5ms/step - AUC: 0.8820 - loss: 0.4274 - val_AUC: 0.0000e+00 - val_loss: 0.4789\n",
      "102/102 - 0s - 5ms/step - AUC: 0.8820 - loss: 0.4274 - val_AUC: 0.0000e+00 - val_loss: 0.4789\n"
     ]
    }
   ],
   "source": [
    "# DEEP NEURAL NETWORK IMPLEMENTATION\n",
    "\n",
    "# Preparing full training set (SMOTE-balanced) and full test set (holdout) for DNN\n",
    "X_train_dnn = X_train_smote.astype('float32').values  # convert to float32 and numpy array\n",
    "y_train_dnn = y_train_smote.astype('float32').values\n",
    "X_test_dnn  = X_test.astype('float32').values\n",
    "y_test_dnn  = y_test.astype('float32').values\n",
    "\n",
    "input_dim = X_train_dnn.shape[1]\n",
    "\n",
    "# Building the Keras model as requested:\n",
    "def build_dnn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(input_dim,)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # binary output\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "dnn_model = build_dnn_model(input_dim)\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the DNN\n",
    "history = dnn_model.fit(\n",
    "    X_train_dnn, y_train_dnn,\n",
    "    validation_split=0.10,  # 10% of training used for validation\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce82c7",
   "metadata": {},
   "source": [
    "## 6. Deep Neural Network Model\n",
    "Build and train a deep neural network with early stopping regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "070d90a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "DNN Confusion Matrix (rows: true class 0/1, cols: predicted 0/1):\n",
      "[[1152  400]\n",
      " [ 133  428]]\n",
      "DNN Confusion Matrix (rows: true class 0/1, cols: predicted 0/1):\n",
      "[[1152  400]\n",
      " [ 133  428]]\n"
     ]
    }
   ],
   "source": [
    "#COMPARATIVE EVALUATION OF MODELS ON TEST SET\n",
    "\n",
    "# Logistic Regression predictions\n",
    "# For LR we used only top10 features\n",
    "y_pred_lr = best_lr.predict(X_test_lr)\n",
    "y_proba_lr = best_lr.predict_proba(X_test_lr)[:, 1]\n",
    "\n",
    "# LR metrics\n",
    "lr_acc = accuracy_score(y_test, y_pred_lr)\n",
    "lr_prec = precision_score(y_test, y_pred_lr)\n",
    "lr_rec = recall_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "lr_auc = roc_auc_score(y_test, y_proba_lr)\n",
    "\n",
    "# DNN predictions\n",
    "y_proba_dnn = dnn_model.predict(X_test_dnn).ravel()\n",
    "y_pred_dnn = (y_proba_dnn >= 0.5).astype(int)\n",
    "\n",
    "# DNN metrics\n",
    "dnn_acc = accuracy_score(y_test_dnn, y_pred_dnn)\n",
    "dnn_prec = precision_score(y_test_dnn, y_pred_dnn)\n",
    "dnn_rec = recall_score(y_test_dnn, y_pred_dnn)\n",
    "dnn_f1 = f1_score(y_test_dnn, y_pred_dnn)\n",
    "dnn_auc = roc_auc_score(y_test_dnn, y_proba_dnn)\n",
    "\n",
    "# Confusion matrix for DNN\n",
    "dnn_cm = confusion_matrix(y_test_dnn, y_pred_dnn)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"DNN Confusion Matrix (rows: true class 0/1, cols: predicted 0/1):\")\n",
    "print(dnn_cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4505d",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "Evaluate both models on the test set using multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93be7eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Comparative Evaluation Results (Test Set / Holdout 30%)\n",
      "\n",
      "                                Model  Accuracy  Precision  Recall  F1-score  ROC-AUC\n",
      "Logistic Regression (top 10 features)    0.7378     0.5039  0.8093    0.6211   0.8354\n",
      "  Deep Neural Network (full features)    0.7478     0.5169  0.7629    0.6163   0.8361\n",
      "\n",
      "Best Logistic Regression hyperparameters found by GridSearchCV: {'C': 100, 'penalty': 'l1'}\n",
      "\n",
      "Top 10 features selected by Mutual Information (pre-SMOTE training set):\n",
      "1. tenure\n",
      "2. Contract_Two year\n",
      "3. InternetService_Fiber optic\n",
      "4. PaymentMethod_Electronic check\n",
      "5. MonthlyCharges\n",
      "6. TotalCharges\n",
      "7. DeviceProtection_No internet service\n",
      "8. TechSupport_No internet service\n",
      "9. StreamingMovies_No internet service\n",
      "10. OnlineBackup_No internet service\n"
     ]
    }
   ],
   "source": [
    "# Building a DataFrame for a clean table\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression (top 10 features)', 'Deep Neural Network (full features)'],\n",
    "    'Accuracy': [lr_acc, dnn_acc],\n",
    "    'Precision': [lr_prec, dnn_prec],\n",
    "    'Recall': [lr_rec, dnn_rec],\n",
    "    'F1-score': [lr_f1, dnn_f1],\n",
    "    'ROC-AUC': [lr_auc, dnn_auc]\n",
    "})\n",
    "\n",
    "# Round metrics for nicer display.\n",
    "results[['Accuracy','Precision','Recall','F1-score','ROC-AUC']] = results[['Accuracy','Precision','Recall','F1-score','ROC-AUC']].round(4)\n",
    "\n",
    "# Show table.\n",
    "print(\"\\n### Comparative Evaluation Results (Test Set / Holdout 30%)\\n\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Also print best LR hyperparameters and top-10 features found earlier\n",
    "print(\"\\nBest Logistic Regression hyperparameters found by GridSearchCV:\", grid.best_params_)\n",
    "print(\"\\nTop 10 features selected by Mutual Information (pre-SMOTE training set):\")\n",
    "for i, feat in enumerate(top10_feature_list, 1):\n",
    "    print(f\"{i}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d4ba0c",
   "metadata": {},
   "source": [
    "## 8. Results Summary\n",
    "Display comparative results and key insights from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b32d523",
   "metadata": {},
   "source": [
    "## Model Persistence\n",
    "Save all trained models and preprocessing objects to disk for future use and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71bf5dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error: The following variables are not defined: best_lr, dnn_model, scaler, smote, grid, RANDOM_STATE, top10_feature_list, X, numeric_cols, categorical_cols, lr_acc, lr_prec, lr_rec, lr_f1, lr_auc, dnn_acc, dnn_prec, dnn_rec, dnn_f1, dnn_auc\n",
      "Please make sure to run all the cells above in order before running this cell.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Check if all required models are defined\n",
    "required_vars = ['best_lr', 'dnn_model', 'scaler', 'smote', 'grid', 'RANDOM_STATE',\n",
    "                 'top10_feature_list', 'X', 'numeric_cols', 'categorical_cols',\n",
    "                 'lr_acc', 'lr_prec', 'lr_rec', 'lr_f1', 'lr_auc',\n",
    "                 'dnn_acc', 'dnn_prec', 'dnn_rec', 'dnn_f1', 'dnn_auc']\n",
    "\n",
    "missing_vars = [var for var in required_vars if var not in locals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"❌ Error: The following variables are not defined: {', '.join(missing_vars)}\")\n",
    "    print(\"Please make sure to run all the cells above in order before running this cell.\")\n",
    "else:\n",
    "    # Creating models directory\n",
    "    models_dir = \"models\"\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "\n",
    "    # Generating timestamp for model versioning\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Saving Logistic Regression model\n",
    "    lr_model_path = os.path.join(models_dir, f\"logistic_regression_model_{timestamp}.pkl\")\n",
    "    with open(lr_model_path, 'wb') as f:\n",
    "        pickle.dump(best_lr, f)\n",
    "    print(f\"✓ Logistic Regression model saved: {lr_model_path}\")\n",
    "\n",
    "    # Saving DNN model (TensorFlow/Keras format)\n",
    "    dnn_model_path = os.path.join(models_dir, f\"dnn_model_{timestamp}.h5\")\n",
    "    dnn_model.save(dnn_model_path)\n",
    "    print(f\"✓ Deep Neural Network model saved: {dnn_model_path}\")\n",
    "\n",
    "    # Saving the scaler\n",
    "    scaler_path = os.path.join(models_dir, f\"minmax_scaler_{timestamp}.pkl\")\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"✓ MinMax Scaler saved: {scaler_path}\")\n",
    "\n",
    "    # Saving SMOTE instance\n",
    "    smote_path = os.path.join(models_dir, f\"smote_instance_{timestamp}.pkl\")\n",
    "    with open(smote_path, 'wb') as f:\n",
    "        pickle.dump(smote, f)\n",
    "    print(f\"✓ SMOTE instance saved: {smote_path}\")\n",
    "\n",
    "    # Saving feature names and metadata\n",
    "    import json\n",
    "    metadata = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"top_10_features\": top10_feature_list,\n",
    "        \"all_features\": X.columns.tolist(),\n",
    "        \"numeric_features\": numeric_cols,\n",
    "        \"categorical_features\": categorical_cols,\n",
    "        \"test_size\": 0.30,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"lr_best_params\": grid.best_params_.copy() if hasattr(grid.best_params_, 'copy') else dict(grid.best_params_),\n",
    "        \"model_performance\": {\n",
    "            \"logistic_regression\": {\n",
    "                \"accuracy\": float(lr_acc),\n",
    "                \"precision\": float(lr_prec),\n",
    "                \"recall\": float(lr_rec),\n",
    "                \"f1_score\": float(lr_f1),\n",
    "                \"roc_auc\": float(lr_auc)\n",
    "            },\n",
    "            \"deep_neural_network\": {\n",
    "                \"accuracy\": float(dnn_acc),\n",
    "                \"precision\": float(dnn_prec),\n",
    "                \"recall\": float(dnn_rec),\n",
    "                \"f1_score\": float(dnn_f1),\n",
    "                \"roc_auc\": float(dnn_auc)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    metadata_path = os.path.join(models_dir, f\"model_metadata_{timestamp}.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    print(f\"✓ Model metadata saved: {metadata_path}\")\n",
    "\n",
    "    print(f\"\\n✅ All models successfully saved in '{models_dir}/' directory!\")\n",
    "    print(f\"\\nSaved Files:\")\n",
    "    print(f\"  - Logistic Regression: {os.path.basename(lr_model_path)}\")\n",
    "    print(f\"  - DNN Model: {os.path.basename(dnn_model_path)}\")\n",
    "    print(f\"  - MinMax Scaler: {os.path.basename(scaler_path)}\")\n",
    "    print(f\"  - SMOTE: {os.path.basename(smote_path)}\")\n",
    "    print(f\"  - Metadata: {os.path.basename(metadata_path)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
